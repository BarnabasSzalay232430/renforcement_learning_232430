{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a6f00c-03db-49e6-a0b5-2f0d6dc64721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 12:58:29.872261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-08 12:58:29.872323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-08 12:58:29.873819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-08 12:58:29.882665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from patchify import patchify, unpatchify\n",
    "from tensorflow.keras import backend as K\n",
    "from skimage.measure import label\n",
    "\n",
    "def f1(y_true, y_pred, threshold=0.3):\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    TP = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    Positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n",
    "    Pred_Positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
    "    precision = TP / (Pred_Positives + tf.keras.backend.epsilon())\n",
    "    recall = TP / (Positives + tf.keras.backend.epsilon())\n",
    "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "    \n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def f1_metric(y_true, y_pred):\n",
    "    return f1(y_true, y_pred, threshold=0.3)\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Weighted binary cross-entropy to address class imbalance.\n",
    "    \"\"\"\n",
    "    # Define weights for foreground (root, shoot, seed) and background\n",
    "    weight_foreground = 10.0\n",
    "    weight_background = 1.0\n",
    "\n",
    "    # Compute weighted binary cross-entropy\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    weights = tf.where(y_true == 1, weight_foreground, weight_background)\n",
    "    loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weights\n",
    "    return K.mean(weighted_loss)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice = (2. * intersection + 1e-7) / (union + 1e-7)\n",
    "    return 1 - dice\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return 0.5 * dice_loss(y_true, y_pred) + 0.5 * weighted_binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6fc810e-99cf-4b09-ac91-c8df749c3f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Root mask predicted.\n",
      "INFO: Connected fragmented root segments.\n",
      "INFO: Detected 98 bounding boxes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 196\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Initialize simulation and environment\u001b[39;00m\n\u001b[1;32m    195\u001b[0m env \u001b[38;5;241m=\u001b[39m OT2Env()  \u001b[38;5;66;03m# Replace with your simulator instance\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[43mfull_pipeline_with_pid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m232430_unet_model_128px_v7md.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 161\u001b[0m, in \u001b[0;36mfull_pipeline_with_pid\u001b[0;34m(sim, model_path, env)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m point \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     mm_coords \u001b[38;5;241m=\u001b[39m convert_to_mm(point, petri_dish\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 161\u001b[0m     robot_coord \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_robot_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmm_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPLATE_POSITION_ROBOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     robot_coords\u001b[38;5;241m.\u001b[39mappend(robot_coord)\n\u001b[1;32m    163\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoot tip robot coordinate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrobot_coord\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m, in \u001b[0;36mconvert_to_robot_space\u001b[0;34m(mm_coords, plate_position_robot)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_robot_space\u001b[39m(mm_coords, plate_position_robot):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Convert mm coordinates to robot space.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmm_coords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplate_position_robot\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (3,) "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from pid_class import PIDController\n",
    "from ot2_class import OT2Env\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"integrated_pipeline_log.txt\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess the input image.\"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def connect_roots(mask):\n",
    "    \"\"\"Dilate the mask to connect fragmented roots.\"\"\"\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))  # Define the dilation kernel\n",
    "    connected_mask = cv2.dilate(mask, kernel, iterations=5)  # Apply dilation\n",
    "    logging.info(\"Connected fragmented root segments.\")\n",
    "    return connected_mask\n",
    "\n",
    "def extract_petri_dish(image):\n",
    "    _, thresholded = cv2.threshold(image, 57, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        logging.warning(\"No contours detected for Petri dish.\")\n",
    "        return image, None\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    return cropped_image, None\n",
    "\n",
    "def predict_root_mask(image, model, patch_size=128, stride=64):\n",
    "    h, w = image.shape\n",
    "    patches = []\n",
    "    positions = []\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(np.stack([patch]*3, axis=-1))\n",
    "            positions.append((y, x))\n",
    "    patches = np.array(patches) / 255.0\n",
    "    predictions = model.predict(patches, verbose=0)\n",
    "    reconstructed = np.zeros((h, w), dtype=np.float32)\n",
    "    counts = np.zeros((h, w), dtype=np.float32)\n",
    "    for pred, (y, x) in zip(predictions, positions):\n",
    "        reconstructed[y:y+patch_size, x:x+patch_size] += pred[..., 0]\n",
    "        counts[y:y+patch_size, x:x+patch_size] += 1\n",
    "    mask = (reconstructed / np.maximum(counts, 1) > 0.5).astype(np.uint8)\n",
    "    logging.info(\"Root mask predicted.\")\n",
    "    return mask\n",
    "\n",
    "def convert_to_mm(pixel_coords, image_shape):\n",
    "    \"\"\"\n",
    "    Convert pixel coordinates to mm-space dynamically using the image shape.\n",
    "    \"\"\"\n",
    "    # Dynamically calculate the conversion factor based on the image shape\n",
    "    plate_size_mm = 150  # Plate size in mm (constant)\n",
    "    h, w = image_shape\n",
    "    conversion_factor_x = plate_size_mm / w  # Conversion factor for the x-axis\n",
    "    conversion_factor_y = plate_size_mm / h  # Conversion factor for the y-axis\n",
    "\n",
    "    # Convert pixel coordinates to mm-space\n",
    "    mm_coords = np.array([\n",
    "        pixel_coords[1] * conversion_factor_x,  # x-coordinate\n",
    "        pixel_coords[0] * conversion_factor_y   # y-coordinate\n",
    "    ])\n",
    "    return mm_coords\n",
    "\n",
    "def convert_to_robot_space(mm_coords, plate_position_robot):\n",
    "    \"\"\"\n",
    "    Convert mm coordinates to robot space.\n",
    "    \"\"\"\n",
    "    return mm_coords + plate_position_robot\n",
    "\n",
    "\n",
    "def find_lowest_point(mask, bounding_box):\n",
    "    x, y, w, h = bounding_box\n",
    "    roi = mask[y:y+h, x:x+w]\n",
    "    coordinates = np.column_stack(np.where(roi > 0))\n",
    "    if len(coordinates) == 0:\n",
    "        return None\n",
    "    lowest_local = coordinates[np.argmax(coordinates[:, 0])]\n",
    "    return lowest_local + [y, x]\n",
    "\n",
    "def measure_bounding_boxes(filtered_mask):\n",
    "    \"\"\"Measure roots with bounding boxes.\"\"\"\n",
    "    contours, _ = cv2.findContours(filtered_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    bounding_boxes = sorted(bounding_boxes, key=lambda b: b[0])  # Sort left-to-right\n",
    "    logging.info(f\"Detected {len(bounding_boxes)} bounding boxes.\")\n",
    "    return bounding_boxes\n",
    "\n",
    "def visualize_results(image, mask, bounding_boxes, robot_coords):\n",
    "    \"\"\"Visualize the predictions and results.\"\"\"\n",
    "    boxed_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h), robot_coord in zip(bounding_boxes, robot_coords):\n",
    "        cv2.rectangle(boxed_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            boxed_image,\n",
    "            f\"({robot_coord[0]:.3f}, {robot_coord[1]:.3f}, {robot_coord[2]:.3f})\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.4,\n",
    "            (0, 255, 0),\n",
    "            1,\n",
    "        )\n",
    "    plt.imshow(boxed_image)\n",
    "    plt.title(\"Detected Roots with Robot Coordinates\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def full_pipeline_with_pid(sim, model_path, env):\n",
    "    \"\"\"Full pipeline with integrated PID control.\"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    image_path = sim.get_plate_image()  # Get image path from simulation\n",
    "    if not os.path.exists(image_path):\n",
    "        logging.error(f\"Image path does not exist: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    image = preprocess_image(image_path)\n",
    "    if image is None:\n",
    "        logging.error(\"Image preprocessing failed.\")\n",
    "        return\n",
    "    \n",
    "    petri_dish, _ = extract_petri_dish(image)\n",
    "    if petri_dish is None:\n",
    "        logging.error(\"Failed to extract Petri dish.\")\n",
    "        return\n",
    "\n",
    "    # Constants\n",
    "    PLATE_SIZE_MM = 150\n",
    "    PLATE_POSITION_ROBOT = np.array([0.10775, 0.088 - 0.026, 0.057])\n",
    "    DEFAULT_Z = 0.057\n",
    "    CONVERSION_FACTOR = PLATE_SIZE_MM / image.shape[0]\n",
    "\n",
    "    # Predictions\n",
    "    predicted_mask = predict_root_mask(petri_dish, model)\n",
    "    connected_mask = connect_roots(predicted_mask)\n",
    "\n",
    "    # Bounding Box Detection\n",
    "    bounding_boxes = measure_bounding_boxes(connected_mask)\n",
    "    robot_coords = []\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        point = find_lowest_point(connected_mask, box)\n",
    "        if point is not None:\n",
    "            mm_coords = convert_to_mm(point, petri_dish.shape)\n",
    "            robot_coord = convert_to_robot_space(mm_coords, PLATE_POSITION_ROBOT)\n",
    "            robot_coords.append(robot_coord)\n",
    "            logging.info(f\"Root tip robot coordinate: {robot_coord}\")\n",
    "        else:\n",
    "            logging.warning(\"No valid lowest point found for bounding box.\")\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_results(petri_dish, connected_mask, bounding_boxes, robot_coords)\n",
    "\n",
    "    # PID Control Loop\n",
    "    for idx, coord in enumerate(robot_coords):\n",
    "        logging.info(f\"Navigating to root tip {idx + 1}: {coord}\")\n",
    "        observation, info = env.reset()\n",
    "        pid_x, pid_y, pid_z = PIDController(), PIDController(), PIDController()\n",
    "        pid_x.setpoint, pid_y.setpoint, pid_z.setpoint = coord\n",
    "\n",
    "        for _ in range(200):\n",
    "            current_position = observation[:3]\n",
    "            error = np.linalg.norm(coord - current_position)\n",
    "            if error < 0.01:\n",
    "                logging.info(f\"Reached root tip {idx + 1} with error {error:.4f}\")\n",
    "                break\n",
    "            action = np.array([\n",
    "                pid_x.compute(current_position[0]),\n",
    "                pid_y.compute(current_position[1]),\n",
    "                pid_z.compute(current_position[2])\n",
    "            ])\n",
    "            observation, _, terminated, _, _ = env.step(action)\n",
    "            if terminated:\n",
    "                logging.warning(\"Simulation terminated unexpectedly.\")\n",
    "                break\n",
    "\n",
    "\n",
    "# Initialize simulation and environment\n",
    "env = OT2Env()  # Replace with your simulator instance\n",
    "full_pipeline_with_pid(sim, \"232430_unet_model_128px_v7md.keras\", env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b934e-0822-4762-8ed6-f50deab651a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
