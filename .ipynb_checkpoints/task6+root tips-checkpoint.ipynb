{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a6f00c-03db-49e6-a0b5-2f0d6dc64721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from patchify import patchify, unpatchify\n",
    "from tensorflow.keras import backend as K\n",
    "from skimage.measure import label\n",
    "\n",
    "def f1(y_true, y_pred, threshold=0.3):\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    TP = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    Positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n",
    "    Pred_Positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
    "    precision = TP / (Pred_Positives + tf.keras.backend.epsilon())\n",
    "    recall = TP / (Positives + tf.keras.backend.epsilon())\n",
    "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "    \n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def f1_metric(y_true, y_pred):\n",
    "    return f1(y_true, y_pred, threshold=0.3)\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Weighted binary cross-entropy to address class imbalance.\n",
    "    \"\"\"\n",
    "    # Define weights for foreground (root, shoot, seed) and background\n",
    "    weight_foreground = 10.0\n",
    "    weight_background = 1.0\n",
    "\n",
    "    # Compute weighted binary cross-entropy\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    weights = tf.where(y_true == 1, weight_foreground, weight_background)\n",
    "    loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_loss = loss * weights\n",
    "    return K.mean(weighted_loss)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice = (2. * intersection + 1e-7) / (union + 1e-7)\n",
    "    return 1 - dice\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return 0.5 * dice_loss(y_true, y_pred) + 0.5 * weighted_binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fc810e-99cf-4b09-ac91-c8df749c3f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OT2Env' object has no attribute 'get_plate_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m env \u001b[38;5;241m=\u001b[39m OT2Env()\n\u001b[1;32m    159\u001b[0m sim \u001b[38;5;241m=\u001b[39m OT2Env()  \u001b[38;5;66;03m# Replace with your simulator instance\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mfull_pipeline_with_pid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m232430_unet_model_128px_v7md.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mfull_pipeline_with_pid\u001b[0;34m(sim, model_path, env)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Full pipeline with integrated PID control.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_path)\n\u001b[0;32m--> 103\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_plate_image\u001b[49m()  \u001b[38;5;66;03m# Get image path from simulation\u001b[39;00m\n\u001b[1;32m    104\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess_image(image_path)\n\u001b[1;32m    105\u001b[0m petri_dish, _ \u001b[38;5;241m=\u001b[39m extract_petri_dish(image)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OT2Env' object has no attribute 'get_plate_image'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from pid_class import PIDController\n",
    "from task10_ot2_gym_wrapper import OT2Env\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"integrated_pipeline_log.txt\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "\n",
    "# Functions\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "    logging.info(f\"Loaded image from: {image_path}\")\n",
    "    return image\n",
    "\n",
    "def extract_petri_dish(image):\n",
    "    _, thresholded = cv2.threshold(image, 57, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        logging.warning(\"No contours detected for Petri dish.\")\n",
    "        return image, None\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    return cropped_image, None\n",
    "\n",
    "def predict_root_mask(image, model, patch_size=128, stride=64):\n",
    "    h, w = image.shape\n",
    "    patches = []\n",
    "    positions = []\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(np.stack([patch]*3, axis=-1))\n",
    "            positions.append((y, x))\n",
    "    patches = np.array(patches) / 255.0\n",
    "    predictions = model.predict(patches, verbose=0)\n",
    "    reconstructed = np.zeros((h, w), dtype=np.float32)\n",
    "    counts = np.zeros((h, w), dtype=np.float32)\n",
    "    for pred, (y, x) in zip(predictions, positions):\n",
    "        reconstructed[y:y+patch_size, x:x+patch_size] += pred[..., 0]\n",
    "        counts[y:y+patch_size, x:x+patch_size] += 1\n",
    "    mask = (reconstructed / np.maximum(counts, 1) > 0.5).astype(np.uint8)\n",
    "    logging.info(\"Root mask predicted.\")\n",
    "    return mask\n",
    "\n",
    "def convert_to_mm(pixel_coords):\n",
    "    \"\"\"Convert pixel coordinates to mm-space.\"\"\"\n",
    "    return np.array(pixel_coords) * CONVERSION_FACTOR\n",
    "\n",
    "def convert_to_robot_space(mm_coords):\n",
    "    \"\"\"Convert mm coordinates to robot space.\"\"\"\n",
    "    return mm_coords + PLATE_POSITION_ROBOT\n",
    "\n",
    "def find_lowest_point(mask, bounding_box):\n",
    "    x, y, w, h = bounding_box\n",
    "    roi = mask[y:y+h, x:x+w]\n",
    "    coordinates = np.column_stack(np.where(roi > 0))\n",
    "    if len(coordinates) == 0:\n",
    "        return None\n",
    "    lowest_local = coordinates[np.argmax(coordinates[:, 0])]\n",
    "    return lowest_local + [y, x]\n",
    "\n",
    "def measure_bounding_boxes(filtered_mask):\n",
    "    \"\"\"Measure roots with bounding boxes.\"\"\"\n",
    "    contours, _ = cv2.findContours(filtered_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    bounding_boxes = sorted(bounding_boxes, key=lambda b: b[0])  # Sort left-to-right\n",
    "    logging.info(f\"Detected {len(bounding_boxes)} bounding boxes.\")\n",
    "    return bounding_boxes\n",
    "\n",
    "def visualize_results(image, mask, bounding_boxes, robot_coords):\n",
    "    \"\"\"Visualize the predictions and results.\"\"\"\n",
    "    boxed_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    for (x, y, w, h), robot_coord in zip(bounding_boxes, robot_coords):\n",
    "        cv2.rectangle(boxed_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            boxed_image,\n",
    "            f\"({robot_coord[0]:.3f}, {robot_coord[1]:.3f}, {robot_coord[2]:.3f})\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.4,\n",
    "            (0, 255, 0),\n",
    "            1,\n",
    "        )\n",
    "    plt.imshow(boxed_image)\n",
    "    plt.title(\"Detected Roots with Robot Coordinates\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def full_pipeline_with_pid(sim, model_path, env):\n",
    "    \"\"\"Full pipeline with integrated PID control.\"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    image_path = sim.get_plate_image()  # Get image path from simulation\n",
    "    image = preprocess_image(image_path)\n",
    "    petri_dish, _ = extract_petri_dish(image)\n",
    "\n",
    "        # Constants\n",
    "    PLATE_SIZE_MM = 150  # Plate size in mm\n",
    "    PLATE_POSITION_ROBOT = np.array([0.10775, 0.088 - 0.026, 0.057])  # Adjusted plate position in robot space\n",
    "    DEFAULT_Z = 0.057  # Default z-coordinate\n",
    "    CONVERSION_FACTOR = PLATE_SIZE_MM / image.shape[0]  # Conversion factor (assume plate size in pixels is 1024)\n",
    "\n",
    "    # Predictions\n",
    "    predicted_mask = predict_root_mask(petri_dish, model)\n",
    "    connected_mask = connect_roots(predicted_mask)\n",
    "\n",
    "    # Bounding Box Detection\n",
    "    bounding_boxes = measure_bounding_boxes(connected_mask)\n",
    "    robot_coords = []\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        point = find_lowest_point(connected_mask, box)\n",
    "        if point is not None:\n",
    "            mm_coords = convert_to_mm(point)\n",
    "            robot_coord = convert_to_robot_space(mm_coords)\n",
    "            robot_coords.append(robot_coord)\n",
    "            logging.info(f\"Root tip robot coordinate: {robot_coord}\")\n",
    "        else:\n",
    "            logging.warning(\"No valid lowest point found for bounding box.\")\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_results(petri_dish, connected_mask, bounding_boxes, robot_coords)\n",
    "\n",
    "    # PID Control Loop\n",
    "    for idx, coord in enumerate(robot_coords):\n",
    "        logging.info(f\"Navigating to root tip {idx + 1}: {coord}\")\n",
    "        observation, info = env.reset()\n",
    "        pid_x, pid_y, pid_z = PIDController(), PIDController(), PIDController()\n",
    "        pid_x.setpoint, pid_y.setpoint, pid_z.setpoint = coord\n",
    "\n",
    "        for _ in range(200):\n",
    "            current_position = observation[:3]\n",
    "            error = np.linalg.norm(coord - current_position)\n",
    "            if error < 0.01:  # Threshold for reaching the target\n",
    "                logging.info(f\"Reached root tip {idx + 1} with error {error:.4f}\")\n",
    "                break\n",
    "            action = np.array([\n",
    "                pid_x.compute(current_position[0]),\n",
    "                pid_y.compute(current_position[1]),\n",
    "                pid_z.compute(current_position[2])\n",
    "            ])\n",
    "            observation, _, terminated, _, _ = env.step(action)\n",
    "            if terminated:\n",
    "                logging.warning(\"Simulation terminated unexpectedly.\")\n",
    "                break\n",
    "\n",
    "# Initialize simulation and environment\n",
    "env = OT2Env()\n",
    "sim = OT2Env()  # Replace with your simulator instance\n",
    "full_pipeline_with_pid(sim, \"232430_unet_model_128px_v7md.keras\", env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b934e-0822-4762-8ed6-f50deab651a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
